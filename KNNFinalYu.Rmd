---
title: "KNNFinalYu"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DT)
library(caret)
library(class)
library(ROCR)
library(MLmetrics)
```

# Data Cleaning
Values with the value "N/A" and "Unknown in the columns bmi and smoking_status were replaced with NA.  All NA values were then removed from the dataset.  The first column, id, was also removed.  Since the data is heavily unbalanced with only roughly 5% of the patients being positive for stroke, the data was sampled to make it more balanced.  All 180 patients who had a stroke were joined with a randomly sampled 180 patients from the original dataset that didn't have a stroke.  
```{r, echo = FALSE, include = FALSE}
stroke <- read.csv("stroke.csv")
stroke$bmi <- gsub("N/A", NA, stroke$bmi)
stroke$smoking_status <- gsub("Unknown", NA, stroke$smoking_status)
stroke <- na.omit(stroke)
stroke <- stroke[ -c(1)]

stroke$gender <- as.factor(stroke$gender)
stroke$ever_married <- as.factor(stroke$ever_married)
stroke$work_type <- as.factor(stroke$work_type)
stroke$Residence_type <- as.factor(stroke$Residence_type)
stroke$smoking_status <- as.factor(stroke$smoking_status)
stroke_positive <- subset(stroke, stroke == 1)
stroke_negative <- subset(stroke, stroke == 0)
set.seed(1980)
stroke_sample <- stroke_negative[sample(1:nrow(stroke_negative), size = 180),]
stroke_final <- rbind(stroke_sample, stroke_positive)

stroke <- lapply(stroke_final, function(x) as.numeric(x))

stroke <- as_tibble(stroke)

```

# Baserate for Dataset
The base-rate for the data set is 50%.
```{r, echo = FALSE, include = FALSE}
mean(as.numeric(stroke$stroke))
```

# Creating Test and Training Set
The test and training set was created with an 80/20 partition where 80% of the data set was used for training and 20% was used for testing. 
```{r, echo = FALSE}
set.seed(1980)
stroke_break <- createDataPartition(stroke$stroke, times = 1, p = 0.8, list = FALSE)
training_stroke <- stroke[stroke_break,]

test_stroke <- stroke[-stroke_break,]
```

# Choosing the best K value for kNN
  Using the elbow method, we determined that a K value of 5 was best for our analysis. 
```{r, echo = FALSE}
chooseK = function(k, train_set, val_set, train_class, val_class){
  set.seed(1)
  class_knn = knn(train = train_set,
                  test = val_set,
                  cl = train_class,
                  k = k,
                  use.all = T)
  conf_mat = table(class_knn, val_class)
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)
  cbind(k = k, accuracy = accu)
}

different_k = sapply(seq(1, 21, by = 2),
                     function(x) chooseK(x,
                                         train_set = training_stroke[, -c(11)],
                                         val_set = test_stroke[, -c(11)],
                                         train_class = training_stroke$stroke,
                                         val_class = test_stroke$stroke
                                         ))

  
different_k  = tibble(k = different_k[1,],
                      recall_sen = different_k[2,])

ggplot(different_k,
       aes(x = k, y = recall_sen)) + 
  geom_line(color = "orange", size = 1.5) + 
  geom_point(size = 3)

```
# KNN Model

The confusion matrix below illustrates some of the the evaluation metrics for the model.  The accuracy of the model overall is 77.78% which is good considering that the base rate for the data set was 50%. The Balanced Accuracy for the model was around 77.78% which is also pretty good compared to the base rate.  The false negative rate was 16.67% which is okay but that means 16.67% of people who had a stroke were classified as not being at risk of a stroke from our model.  The false positive rate was 27.78% which is not great and higher than the false negative but since our model is catered towards trying to catch people who are likely to have a stroke before it happens, it is better to have a higher false positive rate than a high false negative rate. The Kappa value for the model was 0.5556 which indicates moderate agreement. For a model like this, it is good that the false negative rate is lower than the false positive rate since it would be very bad if a patient who was at risk was told that they didn't have to worry about anything. The Log-Loss was calculated to be 2.790974 which is bad since that means the model is not totally confident for a lot of the predictions.  The F1-score was 0.7647059 which is pretty good meaning that the there aren't too many false negatives and false positives. 
```{r, echo = FALSE, include = FALSE}
stroke_KNN <- knn(train = training_stroke[, -c(11)],
              test = test_stroke[, -c(11)],
              cl = training_stroke$stroke,
              k = 5,
              use.all = T,
              prob = T)

prob_stroke<- data.frame(prob = attr(stroke_KNN, "prob"))
```

```{r, echo = FALSE}
stroke_matrix <- confusionMatrix(as.factor(stroke_KNN), as.factor(test_stroke$stroke), positive = "1", dnn = c("Prediction", "Actual"), mode = "sens_spec")

stroke_matrix

```

```{r, echo = FALSE, include = FALSE}
prob_stroke <- prob_stroke %>%
  mutate(prob_stroke, target = as.numeric(stroke_KNN))
prob_stroke$target <- as.numeric(recode(prob_stroke$target, '2' = '1', '1' = '0'))

prob_stroke <- prob_stroke %>%
  mutate('1' = ifelse(prob_stroke$target =='1', prob_stroke$prob, 1-prob_stroke$prob))

prob_stroke <- prob_stroke%>%
  mutate('0' = 1-prob_stroke$`1`)

LogLoss(as.numeric(prob_stroke$`1`), as.numeric(test_stroke$stroke)) # 0.8960143

test_stroke$stroke <- recode(test_stroke$stroke, '0' = 1, '1' = 2)
F1_Score(as.numeric(stroke_KNN), as.numeric(test_stroke$stroke)) # 0.7647059



```

# ROC/AUC Output
The AUC curve had a value of 0.77276273 which is pretty good meaning that our model is doing a good job of distinguishing between patients with and without disease.
```{r, echo = FALSE, include = FALSE}
stroke_eval <- data.frame(pred_class = stroke_KNN, pred_prob = prob_stroke$`1`, target = as.numeric(test_stroke$stroke))
pred_stroke <- prediction(stroke_eval$pred_prob, stroke_eval$target)

stroke_perf <- performance(pred_stroke, "tpr","fpr")
plot(stroke_perf, colorize = TRUE) + 
  abline(a=0,b=1)
stroke_perf_AUC <- performance(pred_stroke, "auc")
print(stroke_perf_AUC@y.values) 

```


